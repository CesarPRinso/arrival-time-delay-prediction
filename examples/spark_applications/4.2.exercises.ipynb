{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2720c580-5cae-4e95-acc8-513899af8603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/11 19:20:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/12/11 19:20:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Context with SparkConf\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "sc = SparkContext.getOrCreate(conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fd9644-c7c8-41c7-b3df-583e432404c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Simón', 'Bolivar', 'VEN', '+58 489 895 965')\n",
      "('Fidel', 'Castro', 'CU', '+53 956 268 348')\n",
      "('Jose', 'Doroteo', 'MEX', '+52 985 621 444')\n",
      "('Ernesto', 'Guevara', 'AR', '+54 895 325 481')\n",
      "('Hugo', 'Chávez', 'VE', '+58 489 895 965')\n",
      "('Camilo', 'Cienfuegos', 'CUB', '+53 956 268 348')\n",
      "('Andrés Manuel', 'López', 'ME', '+52 985 621 444')\n",
      "('Juan Domingo', 'Perón', 'ARG', '+54 985 621 444')\n"
     ]
    }
   ],
   "source": [
    "# Excercise 1\n",
    "# Add the phone prefix to the numbers using as reference the International Calling Codes\n",
    "# Use a Broadcast Variable\n",
    "\n",
    "input_data = [(\"Simón\", \"Bolivar\", \"VEN\", \"489 895 965\"),\n",
    "              (\"Fidel\", \"Castro\", \"CU\", \"956 268 348\"),\n",
    "              (\"Jose\", \"Doroteo\", \"MEX\", \"985 621 444\"),\n",
    "              (\"Ernesto\", \"Guevara\", \"AR\", \"895 325 481\"),\n",
    "              (\"Hugo\", \"Chávez\", \"VE\", \"489 895 965\"),\n",
    "              (\"Camilo\", \"Cienfuegos\", \"CUB\", \"956 268 348\"),\n",
    "              (\"Andrés Manuel\", \"López\", \"ME\", \"985 621 444\"),\n",
    "              (\"Juan Domingo\", \"Perón\", \"ARG\", \"985 621 444\"),\n",
    "              ]\n",
    "\n",
    "rdd = sc.parallelize(input_data)\n",
    "calling_codes = {\"VEN\": \"+58\", \"CU\": \"+53\", \"MEX\": \"+52\", \"AR\": \"+54\", \"VE\": \"+58\", \"CUB\": \"+53\", \"ME\": \"+52\",\n",
    "                 \"ARG\": \"+54\"}\n",
    "broadcast_codes = sc.broadcast(calling_codes)\n",
    "\n",
    "\n",
    "def add_prefix(record):\n",
    "    first_name, last_name, country_code, phone_number = record\n",
    "    prefix = broadcast_codes.value.get(country_code, \"\")\n",
    "    return (first_name, last_name, country_code, f\"{prefix} {phone_number}\")\n",
    "\n",
    "\n",
    "result_rdd = rdd.map(add_prefix)\n",
    "result = result_rdd.collect()\n",
    "for record in result:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf29d59-1e03-4100-ac7f-d14c24802e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8203980-5c79-4ea4-aa71-89116ed64048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines: 30383\n",
      "Number of occurrences of 'to': 12837\n"
     ]
    }
   ],
   "source": [
    "# Excercise 2\n",
    "# Count the number of times the word 'to' appears in a line and the number of lines in the bible.txt file\n",
    "# Use Accumulators\n",
    "\n",
    "def count_to_and_lines(line):\n",
    "    global line_count, to_count\n",
    "    words = line.split()\n",
    "    to_count += words.count('to')\n",
    "    line_count += 1\n",
    "\n",
    "\n",
    "input_file_path = \"../../data/spark_applications/bible.txt\"\n",
    "\n",
    "# Crear un acumulador para contar las líneas\n",
    "line_count = sc.accumulator(0)\n",
    "\n",
    "to_count = sc.accumulator(0)\n",
    "\n",
    "rdd = sc.textFile(input_file_path)\n",
    "rdd.foreach(count_to_and_lines)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(f\"Number of lines: {line_count.value}\")\n",
    "print(f\"Number of occurrences of 'to': {to_count.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2cb4bb-b69f-408a-b911-490fb3354c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80da84d3-032d-4a1a-a122-9b15f3b19b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Excercise 3\n",
    "# Write the RDD containing the pagecounts dataset \n",
    "# Write the RDD but with only 2 partitions+\n",
    "# Use Repartition\n",
    "import shutil\n",
    "\n",
    "input_file_path = \"../../data/spark_applications/pagecounts\"\n",
    "rdd = sc.textFile(input_file_path)\n",
    "rdd_repartitioned = rdd.repartition(2)\n",
    "# Delete the existing output directory if it exists\n",
    "output_file_path = \"pagecounts_repartitioned\"\n",
    "shutil.rmtree(output_file_path, ignore_errors=True)\n",
    "\n",
    "rdd_repartitioned.saveAsTextFile(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8aebe-544d-451a-9e60-336ca2670ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f36a976-62ce-491f-887c-76e3f4ddb1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================================>                         (4 + 3) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines without cache: 4729148\n",
      "Time without cache: 1.4901230335235596 seconds\n",
      "\n",
      "Number of lines with cache: 4729148\n",
      "Time with cache: 2.2003750801086426 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Excercise 4\n",
    "# Check the differences in computation time when using cache method on an RDD\n",
    "# Read pagecount files and count lines with and without using cache method\n",
    "# show the time differences\n",
    "# Use Cache\n",
    "\n",
    "import time\n",
    "input_file_path = \"../../data/spark_applications/pagecounts\"\n",
    "rdd = sc.textFile(input_file_path)\n",
    "\n",
    "# Measure time without caching\n",
    "start_time_without_cache = time.time()\n",
    "\n",
    "# Count the number of lines without caching\n",
    "count_without_cache = rdd.count()\n",
    "\n",
    "end_time_without_cache = time.time()\n",
    "time_without_cache = end_time_without_cache - start_time_without_cache\n",
    "\n",
    "# Cache the RDD\n",
    "rdd.cache()\n",
    "\n",
    "# Measure time with caching\n",
    "start_time_with_cache = time.time()\n",
    "\n",
    "# Count the number of lines with caching\n",
    "count_with_cache = rdd.count()\n",
    "\n",
    "end_time_with_cache = time.time()\n",
    "time_with_cache = end_time_with_cache - start_time_with_cache\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of lines without cache: {count_without_cache}\")\n",
    "print(f\"Time without cache: {time_without_cache} seconds\")\n",
    "\n",
    "print(f\"\\nNumber of lines with cache: {count_with_cache}\")\n",
    "print(f\"Time with cache: {time_with_cache} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51523803-deea-4aaa-b240-47e75bce483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/11 19:43:57 INFO SparkContext: Running Spark version 3.5.0\r\n",
      "23/12/11 19:43:57 INFO SparkContext: OS info Mac OS X, 13.3.1, x86_64\r\n",
      "23/12/11 19:43:57 INFO SparkContext: Java version 17.0.7\r\n",
      "23/12/11 19:43:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "23/12/11 19:43:57 INFO ResourceUtils: ==============================================================\r\n",
      "23/12/11 19:43:57 INFO ResourceUtils: No custom resources configured for spark.driver.\r\n",
      "23/12/11 19:43:57 INFO ResourceUtils: ==============================================================\r\n",
      "23/12/11 19:43:57 INFO SparkContext: Submitted application: app.py\r\n",
      "23/12/11 19:43:57 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\r\n",
      "23/12/11 19:43:57 INFO ResourceProfile: Limiting resource is cpu\r\n",
      "23/12/11 19:43:57 INFO ResourceProfileManager: Added ResourceProfile id: 0\r\n",
      "23/12/11 19:43:57 INFO SecurityManager: Changing view acls to: cesarpantoja\r\n",
      "23/12/11 19:43:57 INFO SecurityManager: Changing modify acls to: cesarpantoja\r\n",
      "23/12/11 19:43:57 INFO SecurityManager: Changing view acls groups to: \r\n",
      "23/12/11 19:43:57 INFO SecurityManager: Changing modify acls groups to: \r\n",
      "23/12/11 19:43:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: cesarpantoja; groups with view permissions: EMPTY; users with modify permissions: cesarpantoja; groups with modify permissions: EMPTY\r\n",
      "23/12/11 19:43:57 INFO Utils: Successfully started service 'sparkDriver' on port 60141.\r\n",
      "23/12/11 19:43:57 INFO SparkEnv: Registering MapOutputTracker\r\n",
      "23/12/11 19:43:58 INFO SparkEnv: Registering BlockManagerMaster\r\n",
      "23/12/11 19:43:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\r\n",
      "23/12/11 19:43:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\r\n",
      "23/12/11 19:43:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\r\n",
      "23/12/11 19:43:58 INFO DiskBlockManager: Created local directory at /private/var/folders/ws/756h0xnn7852yptx9wp_63600000gn/T/blockmgr-2bd41a6e-ad28-423c-9c36-2691f1cb30a8\r\n",
      "23/12/11 19:43:58 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\r\n",
      "23/12/11 19:43:58 INFO SparkEnv: Registering OutputCommitCoordinator\r\n",
      "23/12/11 19:43:58 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\r\n",
      "23/12/11 19:43:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\r\n",
      "23/12/11 19:43:58 INFO Utils: Successfully started service 'SparkUI' on port 4041.\r\n",
      "23/12/11 19:43:58 INFO Executor: Starting executor ID driver on host 10.150.107.151\r\n",
      "23/12/11 19:43:58 INFO Executor: OS info Mac OS X, 13.3.1, x86_64\r\n",
      "23/12/11 19:43:58 INFO Executor: Java version 17.0.7\r\n",
      "23/12/11 19:43:58 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\r\n",
      "23/12/11 19:43:58 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@63a2edec for default.\r\n",
      "23/12/11 19:43:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60142.\r\n",
      "23/12/11 19:43:58 INFO NettyBlockTransferService: Server created on 10.150.107.151:60142\r\n",
      "23/12/11 19:43:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\r\n",
      "23/12/11 19:43:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.150.107.151, 60142, None)\r\n",
      "23/12/11 19:43:58 INFO BlockManagerMasterEndpoint: Registering block manager 10.150.107.151:60142 with 434.4 MiB RAM, BlockManagerId(driver, 10.150.107.151, 60142, None)\r\n",
      "23/12/11 19:43:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.150.107.151, 60142, None)\r\n",
      "23/12/11 19:43:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.150.107.151, 60142, None)\r\n",
      "Number of EN pages: 2245124\r\n",
      "('en', '!', 13, 307446)\r\n",
      "('en', '!!', 2, 14565)\r\n",
      "('en', '!!!', 23, 356424)\r\n",
      "('en', '!!!Fuck_You!!!', 6, 58373)\r\n",
      "('en', '!!!Fuck_You!!!_and_Then_Some', 1, 10185)\r\n",
      "('en', '!!!_(album)', 4, 29794)\r\n",
      "('en', '!!Destroy-Oh-Boy!!', 1, 38670)\r\n",
      "('en', '!%3D', 2, 14116)\r\n",
      "('en', '!5_The_Blood', 1, 6176)\r\n",
      "('en', '!Action_Pact!', 2, 18003)\r\n",
      "('en', 'Special:Random', 405305, 218224631)\r\n",
      "('en', 'Special:Search', 222902, 561104989)\r\n",
      "('en', 'Main_Page', 222302, 5025224410)\r\n",
      "('en', '404_error', 42051, 135095134)\r\n",
      "('en', 'Special:Export/Wikipedia', 32765, 107400209)\r\n",
      "('en', 'Jersey_Shore_(TV_series)', 8954, 246065876)\r\n",
      "('en', 'Jackass_3D', 5231, 98571406)\r\n",
      "('en', 'Arcade_Fire', 5105, 128945542)\r\n",
      "('en', 'Wiki', 4841, 215756850)\r\n",
      "('en', 'United_States', 4694, 2160246570)\r\n"
     ]
    }
   ],
   "source": [
    "# Excercise 5\n",
    "# use spark-submit to launch the app.py file by yourself\n",
    "# :)\n",
    "\n",
    "!spark-submit --master local app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d36129-0b5f-4c19-9f6d-f2f24cb38513",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
